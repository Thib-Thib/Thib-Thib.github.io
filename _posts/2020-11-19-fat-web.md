---
layout: post
title: "Fat Web"
author: Thib
tags: Web
image: images/website.svg
---

Une page de texte requiert de 6 à 55 pages de code HTML, selon Frederic Filloux, rédacteur dans la chronique du [Mondaynote](https://mondaynote.com/bloated-html-the-best-and-the-worse-cac6eb06496d#.pnn9sqz35) en 2016. Ses propos sont illustrés notamment avec un article du Gardian sur le [bannissement des athlètes russes lors des jeux paralympiques à Rio](https://www.theguardian.com/sport/2016/aug/06/russian-athletes-banned-paralympic-games-rio). En quelques chiffres, l’article contient 757 mots (4 667 caractères) contre 485 527 caractères de code HTML. Autrement dit, le code HTML ne contient que 0,96% de texte utile (lisible par l’humain).

# Une goutte dans un verre d’eau

Aujourd’hui j’ai effectué ces tests pour comparer ; selon [pingdom](https://tools.pingdom.com/) et depuis le serveur allemand, la page pèse 2,4MB et l’article contient 761 mots (pour 4 612 caractères) contre 775 780 caractères de code HTML (exporté via le code source de la page). Le code HTML a gonflé de plus de 60% en 4 ans, et le ratio de texte utile est à présent de 0,09% ! L’article n’a pourtant pas changé.

On reproduit le même test pour un article au hasard du Monde sur [Trump au sommet de l’APEC](https://www.lemonde.fr/international/article/2020/11/20/donald-trump-niant-toujours-sa-defaite-doit-intervenir-au-sommet-de-l-apec_6060464_3210.html) et le constat est plutôt similaire : la page pèse 1,6MB, l’article contient 888 mots (5 319 caractères), tandis que sa page HTML compte 119 198 caractères de code. Malgré la baisse de code HTML, le ratio reste surprenant : 4% environ, plus faible encore que la moyenne arbitraire des tests de Frederic Filloux (cf. [mondaynote](https://mondaynote.com/bloated-html-the-best-and-the-worse-cac6eb06496d#.pnn9sqz35)).

![Image with caption]({{site.baseurl}}/images/comparaison.jpg)
_À gauche l’article du Monde, à droite le code associé_


La **crise de l’obésité du web** (*web obesity crisis*) est un phénomène relativement peu connu qui concerne l’augmentation constante du poids des pages Web. Selon [httparchive](https://httparchive.org/reports/state-of-the-web), le poids médian d’une page aujourd’hui (octobre 2020) est d’environ 2MB (2062KB) pour ordinateur et 1,8MB (1891KB) pour mobile, représentant respectivement une augmentation de 341% et de 1206% en 10 ans !

On note également que les calculs effectués tiennent compte de l’ensemble des imports des ressources requises par la page Web.

![Image with caption]({{site.baseurl}}/images/weight_web3.png)
_Évolution du poids moyen (en KB) des sites web selon httparchive.org_


Les contenus (lisibles par l’homme) ont peu changé, alors quelles sont ces ressources nécessaires à l’affichage d’une page ?

# L’envers du menu

Lors d’une conférence en 2015 de Web Directions intitulé « [The Website Obesity Crisis](https://www.webdirections.org/blog/the-website-obesity-crisis/) », l’entrepreneur et développeur web [Maciej Cegłowski](https://en.wikipedia.org/wiki/Maciej_Ceg%C5%82owski) explique que la composition d’un site Web peut se résumer de la façon suivante :

- Une base de HTML, composée de l’article humainement lisible
- Un « énorme tas de déchets », rempli d’informations non pertinentes, d’images lourdes, de publicité, etc.
- De scripts de surveillance, notamment avec les traqueurs en JavaScript

![Image with caption]({{site.baseurl}}/images/pyramid_compressed.jpg)
_« Web pyramid as we observe it in the wild » <cite>&mdash; Maciej Cegłowski </cite>_

Retrouve t-on ces composants dans l’article du Gardian ?

L’outil [pingdom](https://tools.pingdom.com/) permet d’observer la performance et la constitution d’un site web. Ci-dessous les résultats détaillés sur la répartition de poids de l’article :

![Image with caption]({{site.baseurl}}/images/gardian_size_type.png)
_Statistiques de répartition de poids pour la page Web du Gardian selon pingdom_

Plus de la moitié du poids de la page Web concerne les scripts ! Que contiennent-ils ?

# À chacun ses goûts

Le cas de l’article du Gardian illustre facilement cette montée en puissance des scripts. Ce sont des morceaux de code qui effectuent des requêtes aux serveurs. Ils permettent notamment de jouer des animations, lancer des vidéos automatiquement, collecter des informations personnelles (métadonnées), etc.

Certains de ces scripts sont également connus sous le nom de traqueur. Ils s’immiscent partout, du simple formulaire (pour récupérer le sexe ou la date de naissance, …) jusqu’aux boutons de réseaux sociaux (j’aime, partager, …), actifs même sans cliquer. Ils assimilent et analysent les cookies des sites visités, définissent un profil de client type et permettent la présentation de publicité ciblée.

![Image with caption]({{site.baseurl}}/images/pub_cible.jpg)
_« Depuis l’été 2020, les chaînes de télévision française peuvent diffuser des publicités ciblées en fonction des données personnelles des spectateurs » (selon [LeMonde](https://www.lemonde.fr/pixels/article/2020/09/05/tout-comprendre-sur-la-publicite-ciblee-qui-s-invite-peu-a-peu-a-la-television_6051051_4408996.html))_

De nombreuses entreprises investissent sur l’utilisation de traqueurs toujours plus efficaces et malheureusement parfois peu enclin au respect de la vie privée. Selon [Statista](https://fr.statista.com/statistiques/489398/part-publicites-numeriques-depenses-publicitaires-medias-france/), la part des dépenses publicitaires médias dédiée à la publicité digitale en France a jusqu’à doublé entre 2010 et 2017 pour atteindre 34%.

Ce phénomène assez peu surprenant s’accorde avec le fonctionnement de notre société de consommation. La publicité est d’autant plus rentable qu’elle est ciblée, d’où l’importance de l’efficacité des traqueurs. Plus on vous connaît, plus les propositions sont pertinentes. En dehors des controverses via la vie privée, ces innovations permettent une utilisation optimisée de la publicité.

# Des serveurs bousculés

Une autre problématique concerne le type des sites web. On en distingue deux :

- Le site **statique**, constitué exclusivement de fichiers HTML et CSS stockés sur un serveur. Ces fichiers sont transmis du serveur au navigateur (vous) pour l’affichage de la page.
- Le site **dynamique** est également constitué de fichiers HTML, CSS mais possède en plus une base de donnée, qui sera interprétée par le serveur à l’aide d’un langage serveur (exemple : PHP). Le serveur génère lui-même la page HTML à partir du modèle qu’on lui confie, ce qui permet de modifier l’affichage d’information de la page en fonction de l’utilisateur.

![Image with caption]({{site.baseurl}}/images/page-statique-et-page-dynamique.png)
_Différence entre une page Web statique et dynamique selon [imedia](https://www.imedias.pro/cours-en-ligne/web-internet/page-web-page-internet/pages-statiques-et-dynamiques/)_

Les **sites statiques** sont très performants par leur vitesse et leur fiabilité. Cependant, ils sont souvent délaissés à l’instar des **sites dynamiques** qui offrent plus d’interactivité avec l’utilisateur par le biais de login, commentaires, formulaires, etc. Environ 40% des sites web sont aujourd’hui créés à partir du seul **gestionnaire de contenu** WordPress (**CMS**).

Les sites dynamiques traitent plus d’informations qu’un site statique, ce qui peut s’observer via le nombre de [requêtes HTTP](https://assiste.com/Requete_HTTP.html) envoyées au serveur, qui les analyse pour ensuite fabriquer une réponse adaptée. L’analyse pingdom sur l’article du Gardian détaille ce nombre de requêtes :

![Image with caption]({{site.baseurl}}/images/gardian_requests_type.png)
_Statistique du nombre de requête envoyé aux serveurs pour l’article du Gardian selon pingdom_


La majorité des requêtes employés permettent ici de récupérer des images (78, par exemple pour illustrer d’autres articles), ainsi que des scripts (63) pour collecter des informations sur l’utilisateur. À noter que des **cookies** sont téléchargés pour chaque script, conservant des données utilisateurs pour faciliter la navigation.

Ainsi, les **sites dynamiques** demandent donc beaucoup plus de travail aux serveurs. Cette évolution croissante des requêtes qu’on leur soumet augmente la complexité de leur gestion et les rend d’autant plus vulnérables aux cyberattaques. Les coûts en électricité pour les maintenir (climatisation, contrôle de l’air, surveillance, …) sont d’autant plus considérables qu’ils nécessitent de plus en plus d’espace. Il faut alors les renforcer, les optimiser et les agrandir.

Une majorité d’utilisateur n’a pas réellement besoin d’un site dynamique en termes fonctionnels. Si la finalité consiste en des pages simples comme un site vitrine d’entreprise, un profil personnel ou un blog, des solutions alternatives existent. On retrouve par exemple des **générateurs de sites statiques** tels que [Jekyll](https://jekyllrb.com/) ou [Hugo](https://gohugo.io/).

# Réétudier son régime

La crise de l’obésité du Web est encore trop peu discutée aujourd’hui. L’un des moyens pour le préserver serait de simplifier au maximum la production des sites. Cependant, les besoins croissants de fidélisation des utilisateurs requièrent toujours plus de ressources. Une prise de conscience est nécessaire pour responsabiliser et comprendre les risques liés au web, mais semble difficile dans notre société de consommation.
